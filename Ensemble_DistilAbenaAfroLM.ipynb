{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install requsite libraries and import all necessary packages"
      ],
      "metadata": {
        "id": "CNSbtAj3wQFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUMMz3zYvsJs"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import re\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, XLMRobertaModel, XLMRobertaTokenizer"
      ],
      "metadata": {
        "id": "ZYRCtKXZwLJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get GPU for fast training"
      ],
      "metadata": {
        "id": "WC1T4Cn21e3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for GPU training\n",
        "\n",
        "def try_gpu():\n",
        "  return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = try_gpu()"
      ],
      "metadata": {
        "id": "jKPwDywb1eQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation Routines\n",
        "1. Fetch Data\n",
        "2. Map Data to appropriate label formats\n",
        "3. Tokenize and encode to indices\n",
        "4. Create train and validation splits\n",
        "5. Create train and validation dataloaders"
      ],
      "metadata": {
        "id": "2GLdwwsZwnVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "\n",
        "def clean_text(text: str):\n",
        "\n",
        "    # lowercase string\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove twitter handles (usernames)\n",
        "    text = re.sub('@[^\\s]+', '', text)\n",
        "\n",
        "    # remove stop words\n",
        "    # text = \" \".join([word for word in str(\n",
        "    #     text).split() if word not in stop_words])\n",
        "\n",
        "    # remove urls\n",
        "    text = re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', text)\n",
        "\n",
        "    # remove punctuations\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # remove repeating characters\n",
        "    text = re.sub(r'(.)1+', r'1', text)\n",
        "\n",
        "    # remove numbers\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    text = re.sub(emoj, '', text)\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "7IzI8H0CwLYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "z0WgkbHK3LEV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "7278d2ea-8e93-4ea5-88a5-25255bb98346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bf1647b42bbd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/SAfriSenti_Dataset/final_setswana_safrisenti_tweets_clean_v3.csv\"\n",
        "#dataset_path = \"final_all_twi_tweets.csv\"\n",
        "# {negative: 0, positive: 1, neutral: 2}\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(dataset_path, sep=\";\", encoding='cp437', engine='python')\n",
        "# Define a mapping for the labels\n",
        "#df.rename(columns={\n",
        " #   'Clean_tweets': 'tweets',\n",
        "#    'Labels': 'labels'\n",
        "#}, inplace=True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "3ihB11D6PqZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentence\"] = df[\"sentence\"].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "Sv51jITqQNDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "df.rename(columns={\n",
        "    'sentence': 'tweets',\n",
        "    'Final_Label': 'labels'\n",
        "}, inplace=True)\n",
        "\n",
        "df\n",
        "\n",
        "# Define a mapping for the labels\n",
        "label_map = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n",
        "\n",
        "# Map the labels to integers using the defined mapping\n",
        "df[\"labels\"] = df[\"labels\"].map(label_map)\n",
        "\n",
        "# Drop rows with NaN labels\n",
        "df = df.dropna(subset=[\"labels\"])\n",
        "\n",
        "# Convert the labels column to integers\n",
        "df[\"labels\"] = df[\"labels\"].astype(int)\n",
        "\n",
        "df[\"tweets\"] = df[\"tweets\"].apply(lambda x: clean_text(x))\n",
        "df"
      ],
      "metadata": {
        "id": "KGfRB0g3wLsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "def preprocess_input(tokenizer, data, max_len):\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,                  # Max length to truncate/pad\n",
        "            padding=\"max_length\",        # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation=True\n",
        "            )\n",
        "\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "m8D7akgUwL8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afroxml_model_path = \"Davlan/afro-xlmr-base\"\n",
        "afrolm_model_path = \"bonadossou/afrolm_active_learning\"\n",
        "\n",
        "#abena_classifier = DistilBertModel.from_pretrained(distilabena)\n",
        "#abena_tokenizer = DistilBertTokenizer.from_pretrained(abena_model_path)\n",
        "afroxml_tokenizer = AutoTokenizer.from_pretrained(afroxml_model_path, do_lower_case=True)\n",
        "afroxlmr_classifier = XLMRobertaModel.from_pretrained(afroxml_model_path)\n",
        "afrolm_tokenizer = XLMRobertaTokenizer.from_pretrained(afrolm_model_path)\n"
      ],
      "metadata": {
        "id": "oJTaZQe0wMQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify max len\n",
        "MAX_LEN = 256\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.tweets.values\n",
        "y = df.labels.values\n",
        "\n",
        "print(df.shape)\n",
        "df.head()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=2020, shuffle=True)\n",
        "\n",
        "# Run function `preprocess_input` on the train set and the validation set\n",
        "# Obtain splits for both DistilAbena and AfroXLM-R\n",
        "\n",
        "#abena_train_xs, abena_train_masks = preprocess_input(abena_tokenizer, X_train, MAX_LEN)\n",
        "#abena_val_xs, abena_val_masks = preprocess_input(abena_tokenizer, X_val, MAX_LEN)\n",
        "\n",
        "afrolm_train_xs, afrolm_train_masks = preprocess_input(afrolm_tokenizer, X_train, MAX_LEN)\n",
        "afrolm_val_xs, afrolm_val_masks = preprocess_input(afrolm_tokenizer, X_val, MAX_LEN)"
      ],
      "metadata": {
        "id": "IJ8TeX5BwMiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "#abena_train_data = TensorDataset(abena_train_xs, abena_train_masks, train_labels)\n",
        "#abena_train_sampler = RandomSampler(abena_train_data)\n",
        "#abena_train_dataloader = DataLoader(abena_train_data, sampler=abena_train_sampler, batch_size=batch_size)\n",
        "\n",
        "afrolm_train_data = TensorDataset(afrolm_train_xs, afrolm_train_masks, train_labels)\n",
        "afrolm_train_sampler = RandomSampler(afrolm_train_data)\n",
        "afrolm_train_dataloader = DataLoader(afrolm_train_data, sampler=afrolm_train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "#abena_val_data = TensorDataset(abena_val_xs, abena_val_masks, val_labels)\n",
        "#abena_val_sampler = SequentialSampler(abena_val_data)\n",
        "#abena_val_dataloader = DataLoader(abena_val_data, sampler=abena_val_sampler, batch_size=batch_size)\n",
        "\n",
        "afrolm_val_data = TensorDataset(afrolm_val_xs, afrolm_val_masks, val_labels)\n",
        "afrolm_val_sampler = SequentialSampler(afrolm_val_data)\n",
        "afrolm_val_dataloader = DataLoader(afrolm_val_data, sampler=afrolm_val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "CgOd6cj2wMze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define DistilAbena and AfroXLMR classes for sentiment analysis."
      ],
      "metadata": {
        "id": "R0JuVVZU4e78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilAbenaSenti(nn.Module):\n",
        "  \"\"\"ABENA (BERT) Model for sentiment analysis.\"\"\"\n",
        "\n",
        "  def __init__(self, model_name=afroxml_model_path, dropout=0.2, freeze_abena=False):\n",
        "    super(DistilAbenaSenti, self).__init__()\n",
        "    input_dim, hidden_size, output_dim = 768, 64, 3\n",
        "\n",
        "    # Get pretrained model\n",
        "    self.abena = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "    # Instantiate a one layer feed-forward net for classification\n",
        "    self.sentiment_classifier = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_dim),\n",
        "        #nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    # Freeze ABENA model\n",
        "    if freeze_abena == True:\n",
        "      for param in self.abena.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None):\n",
        "    outputs = self.abena(input_ids, attention_mask)\n",
        "\n",
        "    # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "    last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "    # Feed into classifier to compute logits\n",
        "    logits = self.sentiment_classifier(last_hidden_state_cls)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, checkpoint_path):\n",
        "    model = cls()\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "VC_mIjmrwNHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afrolm_model = XLMRobertaModel.from_pretrained(afrolm_model_path)\n",
        "\n",
        "class AfroLMSenti(nn.Module):\n",
        "  \"\"\"AfroLM Model for sentiment analysis.\"\"\"\n",
        "\n",
        "  def __init__(self, dropout=0.2, freeze=True):\n",
        "    super(AfroLMSenti, self).__init__()\n",
        "    input_dim, hidden_size, output_dim = 768, 64, 3\n",
        "\n",
        "    # Get pretrained model\n",
        "    self.afrolm = afrolm_model\n",
        "\n",
        "    # Instantiate a one layer feed-forward net for classification\n",
        "    self.sentiment_classifier = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_dim),\n",
        "        #nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    # Freeze model\n",
        "    if freeze == True:\n",
        "      for param in self.afrolm.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None):\n",
        "    outputs = self.afrolm(input_ids, attention_mask)\n",
        "\n",
        "    # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "    last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "    # Feed into classifier to compute logits\n",
        "    logits = self.sentiment_classifier(last_hidden_state_cls)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, checkpoint_path):\n",
        "    model = cls()\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "qwQ47QyX4ttw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load finetuned models"
      ],
      "metadata": {
        "id": "OflbR3pZ54Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to finetuned checkpoints\n",
        "afroxlmr_senti_path = \"/content/drive/My Drive/final/afroxlmr2-09.pth\"\"\n",
        "afrolm_senti_path = \"/content/drive/My Drive/final/afroxlmr2-04.pth\""
      ],
      "metadata": {
        "id": "-qW-KxlLQJij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rq66xQScHb6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilabena_senti = DistilAbenaSenti.from_pretrained(afroxlmr_senti_path).to(device)\n",
        "afrolm_senti = AfroLMSenti.from_pretrained(afrolm_senti_path).to(device)"
      ],
      "metadata": {
        "id": "i4Qp_fvPwNgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract features from finetuned models and convert into numpy arrays for XGBoost."
      ],
      "metadata": {
        "id": "eNwNQCORT-ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_predictions(model, dataloader):\n",
        "  model.eval()\n",
        "  all_logits = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_input_ids, batch_attn_masks, _ in dataloader:\n",
        "      # Move to device\n",
        "      batch_input_ids, batch_attn_masks = batch_input_ids.to(device), batch_attn_masks.to(device)\n",
        "      logits = model(batch_input_ids, batch_attn_masks)\n",
        "\n",
        "      # Convert logits to numpy and append to all_logits_list\n",
        "      all_logits.append(logits.cpu().numpy())\n",
        "\n",
        "    all_logits = np.vstack(all_logits)\n",
        "  return all_logits"
      ],
      "metadata": {
        "id": "LlMpnj2HwN0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get models predictions and ensemble with XGBoost."
      ],
      "metadata": {
        "id": "22o8cgqmWcsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model predictions an\n",
        "distilabena_features = get_model_predictions(distilabena_senti, abena_train_dataloader)\n",
        "afrolm_features = get_model_predictions(afrolm_senti, afrolm_train_dataloader)\n",
        "\n",
        "#ensembled_features = np.hstack((distilabena_features, afrolm_features))\n",
        "ensembled_features = (0.4*distilabena_features) + (0.6*afrolm_features)\n",
        "target_labels = y_train"
      ],
      "metadata": {
        "id": "6klJObQ_WGin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit XGB Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    learning_rate=1e-2,\n",
        "    max_depth=32,\n",
        "    colsample_bytree=0.1,\n",
        "    colsample_bylevel=0.2,\n",
        "    colsample_bynode=0.2,\n",
        "    n_estimators=20,\n",
        ")\n",
        "\n",
        "xgb_classifier.fit(\n",
        "    ensembled_features, y_train)"
      ],
      "metadata": {
        "id": "gbpNk2GjXG3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble test set and evaluate XGBoost classifier\n",
        "abenasenti_test_fatures = get_model_predictions(distilabena_senti, abena_val_dataloader)\n",
        "afrolm_test_features = get_model_predictions(afrolm_senti, afrolm_val_dataloader)\n",
        "ensembled_test_features = (0.4 * abenasenti_test_fatures) + (0.6 * afrolm_test_features)\n",
        "\n",
        "preds = {}\n",
        "preds['validation'] = xgb_classifier.predict(ensembled_test_features)"
      ],
      "metadata": {
        "id": "RRWNI_7zZJiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Classifier"
      ],
      "metadata": {
        "id": "3nzHyxLibDAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_val, preds['validation'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report for more detailed evaluation\n",
        "print(classification_report(y_val, preds['validation']))"
      ],
      "metadata": {
        "id": "v-zTRFTmaY7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lz62M1lMbQvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}